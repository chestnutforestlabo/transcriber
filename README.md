# Transcriber ğŸ“ğŸ™ï¸

A toolkit that **automatically transcribes multiâ€‘speaker meetings/interviews** with  
**Whisper v3** (ASR) + **Pyannote** (speaker diarization) and lets you review  
the result in a React frontâ€‘end with waveformâ€‘synchronised captions.

![Screenshot of Transcriber UI](images/scrrenshot.png)

```bash
Project structure
â”œâ”€ audios/num_speakers_N/ # Input audio files (N = max number of speakers)
â”œâ”€ models # this is where the models will be saved as
â”œâ”€ outputs # this is where the transcriptions will be saved at
â”œâ”€ environments
â”‚   â”œâ”€ .env
â”‚   â”œâ”€ envs.env #you need to make this by yourelf
â”‚   â”œâ”€ DockerfileBackend
â”‚   â”œâ”€ DockerfileFrontend
â”‚   â””â”€ docker-compose.yaml
â”œâ”€ scripts/ # Shell scripts
â””â”€ src
    â”œâ”€ backend/ # Inference scripts & model wrappers
    â””â”€ frontend/ # Vite + React web app
```

---

## 0. Prerequisites

| Requirement           | Recommended | Notes                                   |
|-----------------------|-------------|-----------------------------------------|
| Python                | 3.9+        | We use UV for dependency handling   |
| CUDAâ€‘enabled GPU      | optional    | CPU works but will be slow              |
| Docker / DockerÂ Compose| 23.x / v2  | For launching the frontâ€‘end container   |
| HuggingÂ Face token    | required    | *Read* scope is enough                  |

---


### âœ… Environment Variable Setup

ğŸ”§ Save Host UID and GID

Create a script to detect and persist your user and group IDs:

```bash
id -u  # e.g., 1000
id -g  # e.g., 1000
```

Edit your shell config file:

```bash
vim ~/.bash_profile  # Or ~/.bashrc, depending on your shell
```

Add the following lines:

```bash
export HOST_UID=1000  # Replace with output from id -u
export HOST_GID=1000  # Replace with output from id -g
```

Apply changes:

```bash
source ~/.bash_profile
```

ğŸ” Hugging Face Token

Before proceeding, create an environment file:

```bash
vim environments/envs.env
```

Add your Hugging Face token inside the file:

```bash
HF_TOKEN=hf_XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
```

Then, navigate to the Hugging Face webpage of [whisper-large-v3](https://huggingface.co/openai/whisper-large-v3) and [pyannote/speaker-diarization-3.1](https://huggingface.co/pyannote/speaker-diarization-3.1) to get access to these models.

## 1. Add your audios
Put .wav files (16Â kHz recommended) under the folder that encodes the
maximum number of different speakers in the recording, e.g.
audios/num_speakers=2/ for a twoâ€‘speaker conversation.

```bash
# exsample
audios/
â”œâ”€ num_speakers=1/
â”œâ”€ num_speakers=2/
â”‚   â”œâ”€ sample1.wav
â”‚   â””â”€ sample2.wav
â””â”€ num_speakers=3/
```

## 2. Run transcription
Run the transcription script:

```bash
# Transcribe your audios
bash ./scripts/transcribe.sh
```
Transcription results will be saved to:

output/<file>.json
frontend/public/transcripts/<file>.json
The original audio is also copied to frontend/public/audios/, and index.json is autoâ€‘updated for frontâ€‘end use.

ğŸ“ Note:
On first use of a Hugging Face model (e.g., openai/whisper-large-v3), you may be required to agree to its license via the model's Hugging Face page.
Please open the model page in your browser and click "Agree and access" before running transcription.

## 3. Start the frontâ€‘end
Open http://localhost:5173 in your browser.
You should see the waveform, speakerâ€‘coloured captions, and you can seek by
clicking either the text or the waveform.

```bash
# Activate frontend Docker container and Activate local server
bash ./scripts/frontend.sh
```

# Contributors
- **Project Lead/Engineer**: [@chestnutforestlabo](https://github.com/chestnutforestlabo)
- **Project Engineer**: [@Shinceliry](https://github.com/Shinceliry)

**ğŸª‚ This project is based on [cvpaperchallenge/Ascender](https://github.com/cvpaperchallenge/Ascender).**